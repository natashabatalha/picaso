{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6da004-2fb1-47fe-b934-0e2ed17c2369",
   "metadata": {},
   "source": [
    "# Implemenation: Grid Fitting+Post-Processing using Bayesian Statistics w/ PICASO\n",
    "\n",
    "If this is your first crack at fitting parameters with PICASO, we strong encourage you to look at the first two retrieval tutorial. Here we will expand on that tutorial and you will learn: \n",
    "\n",
    "**What you will learn:** \n",
    "\n",
    "1. How to load a grid and post-process clouds\n",
    "    - Instead of interpolating spectra like in the previous tutorial we are going to interpolate the temperature and chemistry \n",
    "2. How to analyze grid fitting + clouds results\n",
    "\n",
    "**What you should know:**\n",
    "\n",
    "1. `picaso`'s retrieval class structure (model_set, prior_set, param_set)\n",
    "2. `picaso.retrieval.create_template` function to generate template scripts (covered in Retrieval Tutorial 1 & 2)\n",
    "3. `picaso.analyze.prep_gridtrieval` function to simply generate and interpolate on a spectrum\n",
    "4. Running `virga` cloud models `picaso` forward modeling (e.g. computing transmission spectra with `justdoit.inputs`)\n",
    "\n",
    "**What you will need:** \n",
    "\n",
    "We will build on the `grid` fitting tutorial 2, using Grant et al. 2024 data. \n",
    "\n",
    "1. Download [WASP-17b grids here](www.doi.org/10.5281/zenodo.14681144) (and unpacked it is 436 Mb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669e156-5b15-47ca-b25a-33b4a9385397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import picaso.justdoit as jdi\n",
    "pd = jdi.pd\n",
    "import picaso.analyze as lyz\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a1039-5c45-4951-94c7-b07043ebbb71",
   "metadata": {},
   "source": [
    "## Step 1) Develop function to get data\n",
    "\n",
    "(same as tutorial 1 & 2) \n",
    "\n",
    "Let's create a function to pull our data where all we need to do is declare who the data is from and it pulls it for us automatically.\n",
    "\n",
    "Note: this format is only a recommendation and you can change any part of this to fit your needs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca69d4b-d302-453c-acbc-7e0c930dc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    \"\"\"\n",
    "    Create a function to process your data in any way you see fit.\n",
    "    Here we are using the ExoTiC-MIRI data \n",
    "    https://zenodo.org/records/8360121/files/ExoTiC-MIRI.zip?download=1\n",
    "    But no need to download it.\n",
    "\n",
    "    Checklist\n",
    "    ---------\n",
    "    - your function returns a spectrum that will be in the same units as your picaso model (e.g. rp/rs^2, erg/s/cm/cm or other) \n",
    "    - your function retuns a spectrum that is in ascending order of wavenumber \n",
    "    - your function returns a dictionary where the key specifies the instrument name (in the event there are multiple)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: \n",
    "        dictionary key: wavenumber (ascneding), flux or transit depth, and error.\n",
    "        e.g. {'MIRI LRS':[wavenumber, transit depth, transit depth error], 'NIRSpec G395H':[wavenumber, transit depth, transit depth error]}\n",
    "    \"\"\"\n",
    "    dat = xr.load_dataset(jdi.w17_data())\n",
    "    #build nice dataframe so we can easily \n",
    "    final = jdi.pd.DataFrame(dict(wlgrid_center=dat.coords['central_wavelength'].values,\n",
    "                transit_depth=dat.data_vars['transit_depth'].values,\n",
    "                transit_depth_error=dat.data_vars['transit_depth_error'].values))\n",
    "\n",
    "    #create a wavenumber grid \n",
    "    final['wavenumber'] = 1e4/final['wlgrid_center']\n",
    "\n",
    "    #always ensure we are ordered correctly\n",
    "    final = final.sort_values(by='wavenumber').reset_index(drop=True)\n",
    "\n",
    "    #return a nice dictionary with the info we need \n",
    "    returns = {'MIRI_LRS': [final['wavenumber'].values, \n",
    "             final['transit_depth'].values  ,final['transit_depth_error'].values]   }\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa11547-dd36-4add-92a1-f387fbb98244",
   "metadata": {},
   "source": [
    "## Step 2) Load Grid \n",
    "\n",
    "Same as tutorial 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd42e4-150f-443d-aee1-6ef49a1cfac7",
   "metadata": {},
   "source": [
    "Let's point towards the grid locations, create a grid fitter object for them, and prep them. \n",
    "\n",
    "If you are not familiar with `lyz.GridFitter` we encourate you to first become familiar with non-Bayesian grid fitting based on purely maximum chi-sq values. You can play around with this [Grid Search tutorial here](https://natashabatalha.github.io/picaso/notebooks/fitdata/GridSearch.html). \n",
    "\n",
    "The basic premise of `prep_gridtrieval`:\n",
    "- vets and transforms the grid to ensure it's square and interprelate-able\n",
    "- checks that there is a common pressure grid for the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d396a5-51f6-476c-ac6e-293ac28327b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_location ='/data2/models/WASP-17b/spec/zenodo/v1'# should ultimately point to location of all .nc files\n",
    "grid_name = 'cldfree' #for your own book-keeping\n",
    "fitter = lyz.GridFitter(grid_name,grid_location, verbose=True, to_fit='transit_depth', save_chem=True)\n",
    "fitter.prep_gridtrieval(grid_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda60c7-1a5c-440a-b519-99d80d007903",
   "metadata": {},
   "source": [
    "## Step 3) Setup `jdi.inputs` PICASO class\n",
    "\n",
    "This is going to get us ready to create a spectrum with PICASO. This also allows us to set anything that we plan to hold constant during the retrieval. For example, stellar parameters or planet parameters or phase angle might be things included as constant during a retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ecd79-f94d-440b-97d9-30207a89e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonus! because we are loading a planet grid here we can pull properties \n",
    "#direct from one of the xarray files \n",
    "ds = lyz.xr.load_dataset(fitter.list_of_files[grid_name][0])\n",
    "planet_params = eval(ds.attrs['planet_params'])\n",
    "stellar_params = eval(ds.attrs['stellar_params'])\n",
    "\n",
    "#planet properties\n",
    "mass = planet_params['mp']['value']\n",
    "mass_unit =  planet_params['mp']['unit'] #you should always double check what units are stored. here i know the xarra\n",
    "radius =  planet_params['rp']['value']\n",
    "radius_unit =  planet_params['rp']['unit']\n",
    "\n",
    "#stellar properties \n",
    "database = stellar_params['database']\n",
    "t_eff = stellar_params['steff']\n",
    "metallicity = stellar_params['feh']\n",
    "log_g = stellar_params['logg']\n",
    "r_star = stellar_params['rs']['value']\n",
    "r_star_unit = stellar_params['rs']['unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca7dc7-137e-48b6-9382-bea5d36f8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's point to what opacity database we want to use\n",
    "#we will keep this in dictionary format for cases where we might need to use multiple \n",
    "#opacity databases \n",
    "opacity = {'db1':jdi.opannection(wave_range=[5,15],filename_db='/data2/picaso_dbs/R15000/all_opacities_0.3_15_R15000.db')}\n",
    "def setup_planet():\n",
    "    \"\"\"\n",
    "    First need to setup initial parameters. Usually these are fixed (anything we wont be including\n",
    "    in the retrieval would go here).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Must return the full picaso class\n",
    "    \"\"\"\n",
    "    pl = {i:jdi.inputs() for i in opacity.keys()}\n",
    "    #define stellar inputs \n",
    "    for i in opacity.keys(): pl[i].star(opacity[i], t_eff,metallicity,log_g,\n",
    "                                        radius=r_star,\n",
    "                                        database = database,\n",
    "                                        radius_unit = jdi.u.Unit(r_star_unit) )\n",
    "    #define reference pressure for transmission \n",
    "    for i in opacity.keys(): pl[i].approx(p_reference=1)\n",
    "    #define planet gravity \n",
    "    for i in opacity.keys(): pl[i].gravity(mass=mass, mass_unit=jdi.u.Unit(mass_unit),\n",
    "              radius=radius, radius_unit=jdi.u.Unit(radius_unit))\n",
    "    return pl\n",
    "\n",
    "planet = setup_planet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cef03-9462-49ee-95e1-4064790192a6",
   "metadata": {},
   "source": [
    "## Step 4) Param Set\n",
    "\n",
    "Let's add cloud parameters to our set. In this retrieval setup we are going to add a `virga` run to our model. If you are not familiar with `virga` cloud model we suggest you complete those forward modeling tutorials first. Let's fit for these two additional cloud parameters in addition to a radius factor to account for the unknown reference pressure. \n",
    "\n",
    "- xRp: Factor of the radius to account for the unknown reference pressure.\n",
    "- fsed : the sedimentation efficiency\n",
    "- logkzz : logarithm of the eddy diffusion (cm2/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13e3cd-6ca3-4bf5-841c-ecba467f5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get the grid parameters directly from the module load, this will help with setting up our free parameters\n",
    "grid_parameters_unique = fitter.interp_params[grid_name]['grid_parameters_unique']\n",
    "\n",
    "class param_set:\n",
    "    \"\"\"\n",
    "    This is much easier now since it is the grid parameters plus an offset to account for unknown reference pressure\n",
    "    \"\"\"\n",
    "    grid_virga = list(grid_parameters_unique.keys())+['xrp','logfsed','logkzz']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b3b4-5320-406b-9581-562dcc5cacb1",
   "metadata": {},
   "source": [
    "## Step 5) Guesses Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f686b-5bd2-4847-a5b6-581c007afeca",
   "metadata": {},
   "source": [
    "In testing, it is very useful to check that it is grabbing the right parameters before doing a full analysis. This is available  for a sanity check if desired. Let's add cloud parameters to our guesses set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a881c5-15e8-4f9d-a1fe-5881d002501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class guesses_set: \n",
    "    \"\"\"\n",
    "    For our guesses, we can verify things are working by just taking the first instance of each grid point\n",
    "    \"\"\"\n",
    "    #completely random guesses just to make sure it runs\n",
    "    grid_virga=[grid_parameters_unique[i][0]\n",
    "             for i in grid_parameters_unique.keys()] + [1,-1,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597fc70-bdfe-49fc-bc4f-2c07f9c5644c",
   "metadata": {},
   "source": [
    "## Step 6) Model Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d36dd-ad98-44f9-9354-5e18ccf9f42b",
   "metadata": {},
   "source": [
    "Let's set up our grid interpolator using picaso's custom interp to get a pressure temperature profile and chemistry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0214-5a97-4ec7-a3ec-2ba95f333f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are running virga we need to point to the Mieff files. If this is unfamiliar to you \n",
    "#please see the virga tutorials first. \n",
    "\n",
    "virga_mieff_files = '/data/virga_dbs/virga_0,3_15_R300/'\n",
    "class model_set:\n",
    "    \"\"\"\n",
    "    There are several different ways to interpolate onto a grid. Here, we use picaso's fast custom interpolator that \n",
    "    uses a flex linear interpolator and can be used on any type of matrix grid. \n",
    "    \"\"\"     \n",
    "    def grid_virga(cube, return_ptchem=False): \n",
    "        \"\"\"Here we want to interpolate on temperature and chemistry instead of the spectra as we did last time. \n",
    "        We can still use custom interp to do so! \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cube : list\n",
    "            List of parameters sampled from Bayesian sampler \n",
    "        return_ptchem : bool \n",
    "            True/False; Default=False. This is new! \n",
    "            Returns the planet class from the function *without* running a spectrum. \n",
    "            This will enable you to use the kwarg `pressure_bands` in the function `picaso.retrieval.get_evaluations` \n",
    "            The formalism is to return the entire planet class in either dictionary or class format. e.g. \n",
    "            {'db1':picaso.inputs class} or just picaso.inputs class \n",
    "        \"\"\"\n",
    "        # 1. Grab parameters from your cube \n",
    "        final_goal = cube[0:len(grid_parameters_unique.keys())]\n",
    "        ## using this formalism with index is a bit more \"fool-proof\" than relying on yourself to get the index number correct \n",
    "        xrp = cube[param_set.grid_virga.index('xrp')]\n",
    "        ## note here I am removing the log in front of fsed and kzz \n",
    "        fsed = 10**cube[param_set.grid_virga.index('logfsed')]\n",
    "        kzz = 10**cube[param_set.grid_virga.index('logkzz')]\n",
    "\n",
    "        # 2. Reset the mass and radius based on the radius scaling factor\n",
    "        for i in opacity.keys(): planet[i].gravity(mass=mass, mass_unit=jdi.u.Unit(mass_unit),\n",
    "              radius=xrp*radius, radius_unit=jdi.u.Unit(radius_unit)) \n",
    "\n",
    "        #3. Interpolate to get temperature\n",
    "        temp = lyz.custom_interp(final_goal,\n",
    "              fitter,grid_name, to_interp='custom',\n",
    "              array_to_interp=fitter.interp_params[grid_name]['square_temp_grid'])\n",
    "\n",
    "        #4. Interpolate to get chemistry for each molecule \n",
    "        ## lets be intentional about what molecules we include in the retrieval\n",
    "        mols = ['H2','He','CO2','CH4','CO','H2O']\n",
    "\n",
    "        ## setup a dataframe that we will use to add our chemistry to \n",
    "        pressure = fitter.pressure[grid_name][0]\n",
    "        df_chem = pd.DataFrame(dict(pressure=pressure,temperature=temp))\n",
    "        for imol in mols: \n",
    "            #still using the same custom interp function just need to replace square_temp with square_chem\n",
    "            df_chem[imol] = lyz.custom_interp(final_goal,\n",
    "              fitter,grid_name, to_interp='custom',\n",
    "              array_to_interp=fitter.interp_params[grid_name]['square_chem_grid'][imol])\n",
    "\n",
    "        #5. Add chemistry and PT profile to PICASO atmosphere class \n",
    "        for i in opacity.keys(): planet[i].atmosphere(df=jdi.pd.DataFrame(df_chem),verbose=False)    \n",
    "        ## add in kzz which we are setting as a free parameter \n",
    "        for i in opacity.keys(): planet[i].inputs['atmosphere']['profile']['kz'] = kzz\n",
    "\n",
    "        #6. Now we can introduce the virga cloud code \n",
    "        for i in opacity.keys(): planet[i].virga(['SiO2','Al2O3'],virga_mieff_files, \n",
    "                 fsed=fsed, verbose=False,sig=1.2)\n",
    "\n",
    "        if return_ptchem: return planet #KEEP THIS ! This will give us a short cut to test our atmosphere and cloud setup without running the spectrum \n",
    "\n",
    "        #7. Create a spectrum -- note here I am just looping through all opacity dictionaries for cases where we have \n",
    "        #more than one opacity file in the dictionary. \n",
    "        x = []\n",
    "        y = []\n",
    "        for i in opacity.keys(): \n",
    "            out = planet[i].spectrum(opacity[i], calculation='transmission',full_output=True)\n",
    "            x += list(out['wavenumber'])\n",
    "            y += list(out['transit_depth'])\n",
    "\n",
    "        #8. Sort by wavenumber so we know we always pass the correct thing to the likelihood function \n",
    "        combined = sorted(zip(x, y), key=lambda pair: pair[0])\n",
    "        wno = np.array([pair[0] for pair in combined])\n",
    "        spectra = np.array([pair[1] for pair in combined])\n",
    "        \n",
    "        offset={} #no offset needed for this calculation\n",
    "        error_inf={} # let's not add error inf \n",
    "        return wno, spectra,offset,error_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4e125-135f-4bfd-a60f-a744febc8c2d",
   "metadata": {},
   "source": [
    "## Step 6) Prior Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e3ae0-e80e-4e34-acea-0a58fa6263f6",
   "metadata": {},
   "source": [
    "Finally, we are storing all the priors for Ultranest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74043db2-4dec-4622-bded-86d1aa36502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_set:\n",
    "    \"\"\"\n",
    "    Store all your priors. You should have the same exact function names in here as\n",
    "    you do in model_set and param_set\n",
    "\n",
    "    Now we need to add priors for our new parameters, which include xrp, log fsed, and log kzz \n",
    "\n",
    "    Make sure the order of the unpacked cube follows the unpacking in your model \n",
    "    set and in your parameter set. \n",
    "    #pymultinest: http://johannesbuchner.github.io/pymultinest-tutorial/example1.html\n",
    "    \"\"\"   \n",
    "    def grid_virga(cube):\n",
    "        params = cube.copy()\n",
    "        for i,key in enumerate(grid_parameters_unique): \n",
    "            minn = np.min(grid_parameters_unique[key]) \n",
    "            maxx = np.max(grid_parameters_unique[key]) \n",
    "            params[i] = minn + (maxx-minn)*params[i]\n",
    "        \n",
    "        #xrp \n",
    "        minn=0.7;maxx=1.3\n",
    "        i = param_set.grid_virga.index('xrp')\n",
    "        params[i] = minn + (maxx-minn)*params[i]\n",
    "        \n",
    "        #log fsed \n",
    "        i = param_set.grid_virga.index('logfsed')\n",
    "        minn=-1; maxx=1\n",
    "        params[i] = minn + (maxx-minn)*params[i]\n",
    "        \n",
    "        #logkzz\n",
    "        minn=7; maxx=11\n",
    "        i = param_set.grid_virga.index('logkzz')\n",
    "        params[i] = minn + (maxx-minn)*params[i]\n",
    "       \n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7390abd-8600-427d-9bb3-b6ace0daaaa3",
   "metadata": {},
   "source": [
    "## Step 7) Loglikelihood\n",
    "\n",
    "No changes from our simple line example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f656a7-b093-4ab6-b1da-86515b5ec604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(cube):\n",
    "    \"\"\"\n",
    "    Log_likelihood function that ultimately is given to the sampler\n",
    "    Note if you keep to our same formats you will not have to change this code move \n",
    "\n",
    "    Tips\n",
    "    ----\n",
    "    - Remember how we put our data dict, error inflation, and offsets all in dictionary format? Now we can utilize that \n",
    "    functionality if we properly named them all with the right keys! \n",
    "\n",
    "    Checklist\n",
    "    --------- \n",
    "    - ensure that error inflation and offsets are incorporated in the way that suits your problem \n",
    "    - note there are many different ways to incorporate error inflation! this is just one example \n",
    "    \"\"\"\n",
    "    #compute model spectra\n",
    "    resultx,resulty,offset_all,err_inf_all = MODEL(cube) # we will define MODEL below \n",
    "\n",
    "    #initiate the four terms we willn eed for the likelihood\n",
    "    ydat_all=[];ymod_all=[];sigma_all=[];extra_term_all=[];\n",
    "\n",
    "    #loop through data (if multiple instruments, add offsets if present, add err inflation if present)\n",
    "    for ikey in DATA_DICT.keys(): #we will also define DATA_DICT below\n",
    "        xdata,ydata,edata = DATA_DICT[ikey]\n",
    "        xbin_model , y_model = jdi.mean_regrid(resultx, resulty, newx=xdata)#remember we put everything already sorted on wavenumber\n",
    "\n",
    "        #add offsets if they exist to the data\n",
    "        offset = offset_all.get(ikey,0) #if offset for that instrument doesnt exist, return 0\n",
    "        ydata = ydata+offset\n",
    "\n",
    "        #add error inflation if they exist\n",
    "        err_inf = err_inf_all.get(ikey,0) #if err inf term for that instrument doesnt exist, return 0\n",
    "        sigma = edata**2 + (err_inf)**2 #there are multiple ways to do this, here just adding in an extra noise term\n",
    "        if err_inf !=0: \n",
    "            #see formalism here for example https://emcee.readthedocs.io/en/stable/tutorials/line/#maximum-likelihood-estimation\n",
    "            extra_term = np.log(2*np.pi*sigma)\n",
    "        else: \n",
    "            extra_term=sigma*0\n",
    "\n",
    "        ydat_all.append(ydata);ymod_all.append(y_model);sigma_all.append(sigma);extra_term_all.append(extra_term); \n",
    "\n",
    "    ymod_all = np.concatenate(ymod_all)    \n",
    "    ydat_all = np.concatenate(ydat_all)    \n",
    "    sigma_all = np.concatenate(sigma_all)  \n",
    "    extra_term_all = np.concatenate(extra_term_all)\n",
    "\n",
    "    #compute likelihood\n",
    "    loglike = -0.5*np.sum((ydat_all-ymod_all)**2/sigma_all + extra_term_all)\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852941-bbd3-46dd-bf06-42c13aae758f",
   "metadata": {},
   "source": [
    "## Step 7) Check models, likelihoods, priors! \n",
    "\n",
    "It looks like our priors are giving us an offset that is evenly distributed about the data. Looks good! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35859d99-e4e9-4d9f-8827-b937202d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can easity grab all the important pieces now that they are neatly stored in a class structure \n",
    "DATA_DICT = get_data()\n",
    "PARAMS = getattr(param_set,'grid_virga')\n",
    "MODEL = getattr(model_set,'grid_virga')\n",
    "PRIOR = getattr(prior_set,'grid_virga')\n",
    "GUESS = getattr(guesses_set,'grid_virga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3432ba-b417-45c5-8456-d94a306eeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#lets plot the data \n",
    "for ikey in DATA_DICT.keys(): \n",
    "    plt.errorbar(x=DATA_DICT[ikey][0], y=DATA_DICT[ikey][1], yerr=DATA_DICT[ikey][2],color='black',\n",
    "                 marker='o', ls=' ',label='Grant data')\n",
    "\n",
    "ntests = 10 #lets do 10 random tests \n",
    "for i in range(ntests): \n",
    "    cube = np.random.uniform(size=len(PARAMS))\n",
    "    params_evaluations = PRIOR(cube)\n",
    "    loglike = loglikelihood(params_evaluations)\n",
    "    x,y,off,err = MODEL(params_evaluations)\n",
    "    plt.plot(x,y,label=str(i)+str(int(loglike)))\n",
    "\n",
    "guessx,guessy,off,err = MODEL(GUESS)\n",
    "guess_log = loglikelihood(GUESS)\n",
    "plt.plot(guessx,guessy,color='black',label='guess '+ str(int(guess_log)))\n",
    "plt.xlim([1e4/14,1e4/5])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80fd8-8d82-4d87-abb6-a98b24a5ae5e",
   "metadata": {},
   "source": [
    "## Step 8) Run the statistical sampler \n",
    "\n",
    "Now that we are running PICASO we recommend we move entirely to scripts. It is not recommended to run retrievals in notebook. The rest of the notebook assumes that the retrieval has been run and the output exists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab804d2-806b-4f4e-87f1-5d7872e4970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6a294-5074-4e10-9f61-5c474e5ce7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler = ultranest.ReactiveNestedSampler(PARAMS, loglikelihood, PRIOR,resume=True,\n",
    "#                                          log_dir='/data/test/ultranest/grid_virga')\n",
    "#note if you wanted to turn thsi in the notebook and save the output you would add resume and log_dir above to save\n",
    "#result = sampler.run()#adding a small number here just so we can test the results in the notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6228a29-745c-43e7-b470-3e41bb2a700f",
   "metadata": {},
   "source": [
    "# Short cut to get grid fitting + `virga` retrieval template in script form\n",
    "\n",
    "Same as before but now we will add `grid_kwargs` to set some additional features in the `GridFitter`. Note this is only optional and you are always free to change the template manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507eb27b-3baf-4a11-ac21-fc57f45bdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import picaso.retrieval as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5754379-1eed-484b-bb88-191d9e74f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtype='grid_virga' #first lets specify the retrieval type 'grid' \n",
    "sript_name='run_test.py' #speciy a script name \n",
    "sampler_output='/data/test/ultranest/grid_virga'\n",
    "\n",
    "grid_location = '/data2/models/WASP-17b/spec/zenodo/v1' # should ultimately point to location of all .nc files\n",
    "grid_name = 'cldfree' #for your own book-keeping\n",
    "to_fit = 'transit_depth' #this is based on what you want to fit in the xarray files s\n",
    "\n",
    "#opacity and chemistry \n",
    "opacity_filename_db = '/data2/picaso_dbs/R15000/all_opacities_0.3_15_R15000.db'\n",
    "molecules = ['H2','He','H2O','CO','CO2','CH4','H2O']\n",
    "\n",
    "#cloud things\n",
    "virga_mieff_files = '/data/virga_dbs/virga_0,3_15_R300/'\n",
    "cloud_species = ['SiO2','Al2O3']\n",
    "\n",
    "#new\n",
    "grid_kwargs={'grid_location':grid_location,'grid_name':grid_name,'to_fit':to_fit,\n",
    "            'opacity_filename_db':opacity_filename_db, 'molecules':molecules,\n",
    "            'virga_mieff_dir':virga_mieff_files,'cloud_species':cloud_species}\n",
    "\n",
    "pr.create_template(rtype,sript_name,sampler_output,grid_kwargs=grid_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c8334-8487-4c6e-a092-1380050ecec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need help remembering what keys are allowed? \n",
    "pr.allowed_keys['grid_virga']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661bce8-b66f-4885-904e-a9cb41759a55",
   "metadata": {},
   "source": [
    "Open up `run_test.py` and modify what you need. We have marked key areas you might want to modify with \"CHANGEME\"\n",
    "\n",
    "Running with mpiexec with 5 cpu: \n",
    "\n",
    "    >> mpiexec -n 5 python -m mpi4py run_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801ee4b-277c-4d0e-a389-26acdc1c0a58",
   "metadata": {},
   "source": [
    "## Some tricks and tips for once you have a built a script\n",
    "\n",
    "Now that you have a built script we can input it to grab our built models, priors, parameters set, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4d558-3b42-4d2a-b58a-a3e6fcfb518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_test as rt #YOUR OWN unique script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8e5e6-6e63-4b83-8ade-24e7fb2f15d0",
   "metadata": {},
   "source": [
    "Now you can easily grab what you need directly and no need to have this in a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b75985-ee54-49eb-abf8-b019e9021af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DICT = rt.get_data()\n",
    "PARAMS = getattr(rt.param_set,'grid_virga')\n",
    "MODEL = getattr(rt.model_set,'grid_virga')\n",
    "PRIOR = getattr(rt.prior_set,'grid_virga')\n",
    "GUESS = getattr(rt.guesses_set,'grid_virga')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db463520-3406-4abf-b9fe-cb04af7f91b9",
   "metadata": {},
   "source": [
    "We can also see use the fitter class to continue our analysis (which we will do in the following section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de35571-944a-496f-ba0f-7ded89c85c12",
   "metadata": {},
   "source": [
    "# PICASO Analysis Tools from Saved Samples \n",
    "\n",
    "This part relies on having ran the above script for the run_test.py and having output in a directory defined by `sampler_output`\n",
    "## Auto read ultranest results\n",
    "\n",
    "This function gives you back some of the most highly used output products: \n",
    "\n",
    "- `samples_equal`: equally weighted samples from the posterior\n",
    "- `max_logl` : the maximum loglikelihood (otherwise known as the Bayesian evidence)\n",
    "- `max_logl_point` : the set of parameters that is associated with the maximum loglikelihood\n",
    "- `med_intervals` : the 1 sigma median constraint intervals for each of the parameters of interest\n",
    "- `ultranest_out` : the raw ultranest output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2b02b-211c-496a-ad7d-1c1bf2bf39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pr.get_info(sampler_output, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6d607-2676-49e3-a09a-347924c7cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108cf10-9a0f-462e-aca8-7c52c4935ce8",
   "metadata": {},
   "source": [
    "## Get spectra interval bands and evaluate max loglikelihood spectra  \n",
    "\n",
    "- `bands_spectra`: 1,2,3 sigma bands for spectra (contains keys such as `1sig_lo` and `1sig_hi` for the spectra)\n",
    "- `max_logl_spectra`: the spectra evaluated at the max logl point\n",
    "- `max_logl_error_inflation` : if exists, the error inflation associated with the max logl point\n",
    "- `max_logl_offsets` : If exists, the offsets associated with the max logl point\n",
    "- `wavelength`: the wavelength grid in um "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478ca48-386a-4f7e-ad1b-1767b08b37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws=200 #number of evaluations for which to \n",
    "evaluations = pr.get_evaluations(results['samples_equal'], results['max_logl_point'], MODEL, n_draws,\n",
    "                                 regrid=100.0,#spectral resolution to regrid to\n",
    "                                 pressure_bands=['temperature','H2O','CO2','CO','CH4']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46444175-63e6-49ea-8f68-ebe94b7e1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8de9d2-bbe0-420e-8606-868e0eb34080",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(7,5))\n",
    "color_scale =  pr.pals.Blues[3]\n",
    "resolution=100\n",
    "f=pr.plot_spectra_bands(evaluations,color_scale, ax=axs,R=resolution)\n",
    "axs.set_title('Cld Free Model')#add other styles here \n",
    "axs.set_xlim([5,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8f678-e52c-4984-8a5d-6edae511d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "color_scale =  pr.pals.Oranges[3]\n",
    "f=pr.plot_pressure_bands(evaluations,color_scale, ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132c3b0-29e2-4ba5-9238-922d81e2875d",
   "metadata": {},
   "source": [
    "## Get Reduced Chi Squared Statistic of Max Logl spectrum Incl Offsets\n",
    "\n",
    "- `wavenumber`: new regridded wavenumber grid on data axis\n",
    "- `model` : model regridded on data axis\n",
    "- `datay` : data with offset included\n",
    "- `datae` : data error (no error inflation included )\n",
    "- `chisq_per_datapt` : chi squared per data point (DOF=len of data array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec0394-b513-4403-bf9e-9a0a059a3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_info = pr.get_chisq_max(evaluations, DATA_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0d911-f961-4738-b494-23d76e6ee720",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb1d37-ce89-4a89-a060-d719f65ceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1e4/chi_info['wavenumber'], chi_info['model'],color='red',label='max logl model')\n",
    "plt.errorbar(x=1e4/chi_info['wavenumber'], y=chi_info['datay'], yerr=chi_info['datae'], \n",
    "             color='black',marker='o', ls=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b0771-a63d-4d3e-91b0-893895ae6e87",
   "metadata": {},
   "source": [
    "## Get all bundled results in xarray format\n",
    "\n",
    "What does `data_output` do? \n",
    "\n",
    "1. bundles all the median and sigma banded output data into an xarray\n",
    "2. adds all the constraint intervals to your xarray in latex format\n",
    "3. creates some default plots of banded spectra (banded chem if you have created it) and corner plots\n",
    "4. creates a pickle of your equally weighted samples if you have requested it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af54183-c8c2-417a-b58d-1a4d4e842f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/data/test/output/tagname'#name of file (without extension!!) where you want the output returned \n",
    "return_samples=True # do you want a pickle file created of all the equally weighted samples? \n",
    "spectrum_tag = 'transit_depth'#in the xarray what do you want the spectrum called? \n",
    "spectrum_unit= 'cm**2/cm**2'#what are the units of your spectrum? \n",
    "author = \"NE Batalha\"#who did the analysis? this is for the xarray\n",
    "contact=\"natasha.e.batalha@nasa.gov\"#how can you be contacted? this is for the xarray\n",
    "model_description=\"cloud virga SiO2 & Al2O3 grid fit\"#describe your model so people will know what it is \n",
    "code = \"PICASO,Virga,Ultranest\" # what codes did you use for this analysis? \n",
    "\n",
    "bxr=pr.data_output(evaluations, results, chi_info, filename,return_samples=True,\n",
    "                     spectrum_tag=spectrum_tag,spectrum_unit=spectrum_unit,\n",
    "                    author=author,contact=contact,\n",
    "                    model_description=model_description,\n",
    "                    code=code)#,\n",
    "                    #round=[2, 1,1,5,3,2,2,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12687fd-da76-4abc-82aa-d00359dac0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b5617-0b9f-4c44-8796-65b51dbb7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bxr.attrs['intervals_params']\n",
    "#note if you do not like the round errors here you can adjust what these numbers are rounded to with \n",
    "#round kwarg to data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a593ee-379b-4575-a81a-1ab49e065929",
   "metadata": {},
   "source": [
    "## Stylize Plots\n",
    "\n",
    "Our default plots were not great, let's beautify them a little "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8d935-e818-4ebb-a321-95e8d66d0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = results['samples_equal']\n",
    "\n",
    "params =  results['param_names']\n",
    "\n",
    "#create mapper for labels \n",
    "pretty_labels={'cto':'C/O',\n",
    "               'mh':'log M/H [xSolar]',\n",
    "               'heat_redis':'heat redis.',\n",
    "               'xrp':r'$x$R$_p$',\n",
    "               'tint':r'T$_\\mathrm{int}$',\n",
    "               'logfsed':r'log f$_\\mathrm{sed}$',\n",
    "               'logkzz':r'log K$_\\mathrm{zz}$'}\n",
    "\n",
    "#create mapper for ranges ?\n",
    "ranges={'cto':[0.25,1],\n",
    "        'tint':[200,300],\n",
    "        'xrp':[0.7,1.3],\n",
    "        'heat_redis':[0.5,0.9],\n",
    "        'mh':[1.3,1.7],\n",
    "        'logfsed':[-1,1],\n",
    "        'logkzz':[7,11]}\n",
    "\n",
    "ints = eval(bxr.attrs['intervals_params'])#get pretty titles \n",
    "intervals={i:ints[i] for i in results['param_names']}\n",
    "\n",
    "f,a=pr.plot_pair(samples,params,pretty_labels=pretty_labels, ranges=ranges,figsize=(15,15),\n",
    "              intervals=intervals)\n",
    "#could make additional style change using a here \n",
    "f.savefig('/data/output/plot_pair.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690134e9-1e56-46a9-9ec8-ecbba8a6ebb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
