{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6da004-2fb1-47fe-b934-0e2ed17c2369",
   "metadata": {},
   "source": [
    "# Model Setup Script\n",
    "\n",
    "Let's break apart the `model_setup.py` script and see exactly what it does, step-by-step.\n",
    "\n",
    "**NOTE**: This is an explanation with directories that may not align with yours. If you encounter any directory errors in [AnaylzeExoplanet_3_Utranest.ipynb](www.google.com), please edit **model_setup.py**.\n",
    "\n",
    "The goal of this script is to set up all the priors, models, directories and environment needed to run our Ultranest and further analysis.\n",
    "\n",
    "It is expected that you have a basic understanding of baynesian analysis, knowledge of priors and likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1132d-eff2-42fb-bcb7-a927a6d8b370",
   "metadata": {},
   "source": [
    "## Declare Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e784e35-c42d-49d3-961e-1b9829028283",
   "metadata": {},
   "source": [
    "Here, we are grabbing the planet parameters from a google sheet which will make our lives much easier as it will automatically pull the parameters needed.\r\n",
    "\r\n",
    "A google sheet is not the only way of storing and pulling parameters, but it is the method used her\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793d6da0-62eb-44f1-8343-6156c6e2f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import picaso.justdoit as jdi\n",
    "import picaso.analyze as lyz\n",
    "\n",
    "\n",
    "sheet_id = '1R3DlcC25MyfP97DNcbfsy1dNehu20BWal8ePOol9Ftg'\n",
    "sheet_name = '0'\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&gid={sheet_name}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930a1-b56b-4253-8e01-c5bccb01bbf0",
   "metadata": {},
   "source": [
    "## Set Up Opacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964b760-e2d8-4ab2-ae81-cef1fbe41bc2",
   "metadata": {},
   "source": [
    "Define the opacities needed to run the climate grid with.\n",
    "\n",
    "Here we set up opacity as a dictionary incase you want to run multiple opacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8df55-6918-45b0-89a7-8d58467cd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity = {\n",
    "            'db1':jdi.opannection( filename_db=\"/data2/picaso_dbs/R15000/all_opacities_0.3_15_R15000.db\", wave_range=[0.3,15])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa11547-dd36-4add-92a1-f387fbb98244",
   "metadata": {},
   "source": [
    "## Observational Data and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff0157-433a-42a1-865c-7ff798da04dc",
   "metadata": {},
   "source": [
    "We also need to point to the observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd42e4-150f-443d-aee1-6ef49a1cfac7",
   "metadata": {},
   "source": [
    "Let's point towards the grid locations, create a grid fitter object for them, and prep them. \n",
    "\n",
    "The basic premise of `prep_gridtrieval`:\n",
    "- vets and transforms the grid to ensure it's square and interprelate-able\n",
    "- checks that there is a common pressure grid for the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d396a5-51f6-476c-ac6e-293ac28327b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"/data2/models/HAT-P-26b/spec/\"\n",
    "grid_name = 'cldfree'\n",
    "fitter = lyz.GridFitter(grid_name,location, verbose=True, to_fit='fpfs_emission', save_chem=True)\n",
    "fitter.prep_gridtrieval(grid_name)\n",
    "\n",
    "data_dir = '/data2/observations/HAT-P-26b/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1356d9-9a46-4fdc-aef6-7a316888426e",
   "metadata": {},
   "source": [
    "Finally, let's define our grid parameters and create them for the fitter. Also, let's pull our planet/star parameters from the google sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc5471-d2a3-4f7b-a923-6c94e4b39a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_parameters_unique = fitter.interp_params[grid_name]['grid_parameters_unique']\n",
    "grid_points = fitter.interp_params[grid_name]['grid_parameters']\n",
    "\n",
    "\n",
    "success = jdi.pd.read_csv(url)\n",
    "log_g = success.loc[0,'logg']\n",
    "metallicity =success.loc[0,'feh']\n",
    "t_eff = success.loc[0,'st_teff'] #K\n",
    "r_star = success.loc[0,'rstar'] #rsun\n",
    "m_planet = success.loc[0,'pl_mass'] #mjup\n",
    "r_planet =success.loc[0,'pl_rad']  #rjup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a1039-5c45-4951-94c7-b07043ebbb71",
   "metadata": {},
   "source": [
    "## Develop function to get data\n",
    "\n",
    "All these classes are used in the `Analyze Ultranest` notebook and `run_ultranest.py` script. Let's create a function to pull our data where all we need to do is declare who the data is from and it pulls it for us automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca69d4b-d302-453c-acbc-7e0c930dc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(nir_name, miri_name): \n",
    "    \"\"\"\n",
    "    Create a function to process your data in any way you see fit.\n",
    "    You could, for example, want to read in the data and rebin.\n",
    "    \n",
    "    Make sure that this is ordered in ascending WAVENUMBER!\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    To run a spectral retrieval you will need wavenumber, flux, and error. The\n",
    "    exact format will depend on how you create your likelihood function, which is up to you!\n",
    "    \"\"\"\n",
    "    wlgrid_center,rprs_data2,wlgrid_width, e_rprs2=[],[],[],[]\n",
    "\n",
    "    if 'Gressier' in nir_name: \n",
    "        raise Exception('not sure what the data is')\n",
    "    elif \"Wake\" in nir_name: \n",
    "        g395h = jdi.xr.load_dataset(os.path.join(data_dir, 'NIRSpec_G395H_Eclipse',nir_name,\n",
    "                                                 'emission-spectrum-h26-g395h-r100ch4-h1-2aug23_v1-x_y.nc'))\n",
    "                                                 #'emission-spectrum-h26-g395h-10pix-h1-2aug23_v1-x_y.nc'))\n",
    "\n",
    "        wlgrid_center+=    list(g395h['central_wavelength'].values) \n",
    "        rprs_data2+=    list(g395h['eclipse_depth'].values)\n",
    "        wlgrid_width+=    list(g395h['bin_half_width'].values)\n",
    "        e_rprs2+=    list(g395h['eclipse_depth_error'].values)\n",
    "\n",
    "    if 'bristol_dv' in miri_name:\n",
    "        file = os.path.join(data_dir, 'MIRI_LRS_Eclipse',miri_name,'current_versions',\n",
    "                            'dv_hp26_miri_eclipse_0.50_bin_v10.txt')\n",
    "        miri = jdi.pd.read_csv(file, delim_whitespace=True)\n",
    "        \n",
    "        wlgrid_center+=    list(miri['bin_central_wv'].values) \n",
    "        rprs_data2+=    list(miri['eclipse_depth'].values)\n",
    "        wlgrid_width+=    list(miri['bin_half_width'].values)\n",
    "        e_rprs2+=    list(miri['eclipse_depth_err'].values)\n",
    "        \n",
    "    elif 'Cornell' in miri_name: \n",
    "        file = os.path.join(data_dir, 'MIRI_LRS_Eclipse',miri_name,\n",
    "                            'HATP26b_miri_lrs_eclipse_ch14.nc')\n",
    "        miri = jdi.xr.load_dataset(file)        \n",
    "        wlgrid_center+=    list(miri['central_wavelength'].values) \n",
    "        rprs_data2+=    list(miri['eclipse_depth'].values)\n",
    "        wlgrid_width+=    list(miri['bin_half_width'].values)\n",
    "        e_rprs2+=    list(miri['eclipse_depth_error'].values)\n",
    "        \n",
    "    \n",
    "    final = jdi.pd.DataFrame(dict(wlgrid_center=wlgrid_center,\n",
    "                rprs_data2=rprs_data2,\n",
    "                wlgrid_width=wlgrid_width,\n",
    "                e_rprs2=e_rprs2))\n",
    "    \n",
    "    final['wno'] = 1e4/final['wlgrid_center']\n",
    "    final = final.sort_values(by='wno').reset_index(drop=True)\n",
    "\n",
    "    returns = {nir_name+miri_name : [final['wno'].values, \n",
    "             final['rprs_data2'].values  ,final['e_rprs2'].values]   }\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572634d-c9aa-48f8-9b8a-10a7d396babe",
   "metadata": {},
   "source": [
    "## Setting up planet model\n",
    "\n",
    "Next, let's set up our planet model. This is creating the planet model for each opacity we run through and saving it into an array of picaso classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d64733-9e63-41af-ae49-e519256cee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_planet():\n",
    "    \"\"\"\n",
    "    First need to setup initial parameters. Usually these are fixed (anything we wont be including\n",
    "    in the retrieval would go here).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Must return the full picaso class\n",
    "    \"\"\"\n",
    "    pl = {i:jdi.inputs() for i in opacity.keys()}\n",
    "    for i in opacity.keys(): pl[i].star(opacity[i], t_eff,metallicity,log_g,\n",
    "                                        radius=r_star,\n",
    "                                        database = 'phoenix',\n",
    "                                        radius_unit = jdi.u.Unit('R_sun') )\n",
    "    #i am putting these here but in a lot of cases, this should go into the likelihood function\n",
    "    #for example when adjusting radius through xrp factor\n",
    "    #or if fitting for planet mass, radius, or gravity\n",
    "    for i in opacity.keys(): pl[i].approx(p_reference=1)\n",
    "    for i in opacity.keys(): pl[i].gravity(mass=m_planet, mass_unit=jdi.u.Unit('M_jup'),\n",
    "              radius=r_planet, radius_unit=jdi.u.Unit('R_jup'))\n",
    "    for i in opacity.keys(): pl[i].phase_angle(0)\n",
    "    return pl\n",
    "\n",
    "planet_og = setup_planet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366185e-5850-4ef6-9c9d-579ec3e2c601",
   "metadata": {},
   "source": [
    "## Setting up Ultranest\n",
    "\n",
    "This is simply keeping track of what parameters are used in each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cef03-9462-49ee-95e1-4064790192a6",
   "metadata": {},
   "source": [
    "## Param Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e13e3cd-6ca3-4bf5-841c-ecba467f5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `prep_gridtrieval` not found.\n"
     ]
    }
   ],
   "source": [
    "class param_set:\n",
    "    \"\"\"\n",
    "    This is purely for book keeping what parameters you have run in each retrieval.\n",
    "    It helps if you keep variables uniform.\n",
    "    THINGS TO CHECK:\n",
    "    1) Make sure that the variables here match how you are unpacking your cube in the model_set class and prior_set\n",
    "    2) Make sure that the variable names here match the function names in model_set and prior_set\n",
    "    \"\"\"\n",
    "    #line='m,b' #simplest test model = blackbody: blackbody='Tiff'\n",
    "    cld_free = ','.join(grid_parameters_unique.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b3b4-5320-406b-9581-562dcc5cacb1",
   "metadata": {},
   "source": [
    "## Guesses Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f686b-5bd2-4847-a5b6-581c007afeca",
   "metadata": {},
   "source": [
    "In testing, it is very useful to check that it is grabbing the right parameters before doing a full analysis. This is available  for a sanity check if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a881c5-15e8-4f9d-a1fe-5881d002501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class guesses_set: \n",
    "    \"\"\"\n",
    "    Optional! \n",
    "    This is only used if you want to test your initial functions before running \n",
    "    the full retrievals. See script test.py\n",
    "    \"\"\"\n",
    "    #completely random guesses just to make sure it runs\n",
    "    cld_free=[grid_parameters_unique[i][0]\n",
    "             for i in grid_parameters_unique.keys()]# + [-.005,-.005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597fc70-bdfe-49fc-bc4f-2c07f9c5644c",
   "metadata": {},
   "source": [
    "## Model Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d36dd-ad98-44f9-9354-5e18ccf9f42b",
   "metadata": {},
   "source": [
    "Here, we are defining the full model. This is essentially prepping and making it easy to digest for Ultranest's `cube` usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0214-5a97-4ec7-a3ec-2ba95f333f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_set:\n",
    "    \"\"\"100e-6=1e-4\n",
    "    This is your full model set. It will include all the functions you want to test\n",
    "    for a particular data set.\n",
    "    \"\"\"     \n",
    "    def cld_free(cube): \n",
    "        final_goal = cube[0:len(grid_parameters_unique.keys())]\n",
    "        spectra_interp = lyz.custom_interp(final_goal, fitter, grid_name)\n",
    "        micron = fitter.wavelength[grid_name]\n",
    "        wno = 1e4/fitter.wavelength[grid_name] \n",
    "        return wno, spectra_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4e125-135f-4bfd-a60f-a744febc8c2d",
   "metadata": {},
   "source": [
    "## Prior Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e3ae0-e80e-4e34-acea-0a58fa6263f6",
   "metadata": {},
   "source": [
    "Finally, we are storing all the priors for Ultranest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74043db2-4dec-4622-bded-86d1aa36502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_set:\n",
    "    \"\"\"\n",
    "    Store all your priors. You should have the same exact function names in here as\n",
    "    you do in model_set and param_set\n",
    "\n",
    "    Make sure the order of the unpacked cube follows the unpacking in your model \n",
    "    set and in your parameter set. \n",
    "    #pymultinest: http://johannesbuchner.github.io/pymultinest-tutorial/example1.html\n",
    "    \"\"\"   \n",
    "    def cld_free(cube):#,ndim, nparams):\n",
    "        params = cube.copy()\n",
    "        for i,key in enumerate(grid_parameters_unique): \n",
    "            minn = np.min(grid_parameters_unique[key]) \n",
    "            maxx = np.max(grid_parameters_unique[key]) \n",
    "            params[i] = minn + (maxx-minn)*params[i]\n",
    "        return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
